\item \subquestionpoints{18} {\bf Преучење изражајних модела над малобројним подацима}

За преостали део овог задатка биће размотрен мали скуп података (случајан подскуп скупа података који је до сада коришћен) са далеко мањим бројем примера који је дат у следећој датотеци:
%
\begin{center}
	\url{src/featuremaps/small.csv}
\end{center}
%

Биће истражено шта се дешава када број својстава буде већи од броја примера у тренинг скупу. Покренути алгоритам на овом малобројном скупу података користећи следеће пресликавање
\begin{align}
\phi(x) = \left[\begin{array}{c} 1\\ x \\ x^2\\ \vdots \\x^k \end{array}\right]\in \mathbb{R}^{k+1} 
\end{align}
са $k = 1,2,5,10,20$. 

Направити дијаграм у коме је свака од хипотеза представљена различитом кривом, као и у претходним подзадацима. Запазити како се апроксимација над тренинг скупом података мења с порастом $k$. Укључити дијаграме у извештај и укратко прокоментарисати запажања.

\textbf{Напомена:} Феномен који се запажа где модел најпре добро апроксимира тренинг скуп података, а затим одједном ``подивља'' је услед ефекта који се назива и \emph{преучење}. Интуиција коју треба развити јесте да када је количина података за тренирање релативно мала у односу на изражајну моћ породице могућих модела (то јест, класе хипотеза која је у овом конкретном случају случај породица свих полинома степена $k$) долази до преучења.

Грубо говорећи, скуп хипотеза је ``врло флексибилан'' и може се лако натерати да прође кроз све примере, односно тачке на прилично неприродан начин. Другим речима, модел покушава да предвиди чак и шум у тренинг скупу података који не би уопште требало да се предвиђа. Ово по правилу шкоди предвиђању модела на тест примерима. Феномен преучења ће бити детаљније разматран на предавањима када се буде обрађивала теорија учења и компромис између помераја и варијансе.

